{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using text data\n",
    "\n",
    "2. You will often encounter text data during fraud detection\n",
    "\n",
    "During fraud analysis, almost always, there will be some sort of textual data available that you can use to your advantage. It can be emails between the company and its clients, or emails within the company. Descriptions of bank transactions are a good example also. It can be that a client service team leaves notes on a client account. Insurance claims are full of textual data, and it may even be the case that there are recorded telephone conversations. And this list goes on. It is, therefore, important to know how to handle textual data, when working on fraud detection problems.\n",
    "\n",
    "3. Text mining techniques for fraud detection\n",
    "\n",
    "When using text data in fraud analysis, there are multiple ways to use the data. A common application is a word search, to flag any data mentioning certain terms. Sentiment analysis, aka measuring how positive or negative a text is, can be another interesting application, which you could also combine with a word search. More straightforward, you can check whether text data associated with fraud tends to be more positive or negative, relative to the normal text. Topic analysis and counting the frequency of certain words of interest, is another powerful application for fraud detection. Let's talk about this in more detail later. A last way to use text data is to analyze the style of fraud data and search for text that is similar in style to flag for fraud.\n",
    "\n",
    "4. Word search for fraud detection\n",
    "\n",
    "Suppose you want to flag all client transactions that mention a certain gambling company, as this company has received bad press lately. A simple word search on all transactions and client emails can easily filter whether any of your clients mention this company. You can then either use these results as a filter, or a flag on its own, or simply as an additional feature in your machine learning model. You can do all this with a few simple lines of code. Let's have a look at how it's done.\n",
    "\n",
    "5. Word counts to flag fraud with pandas\n",
    "\n",
    "Pandas has functions that allow you to do operations on text data within a pandas series or DataFrame. In this example, I use the string-dot-contains function to find all rows that contain the words money laundering. You can very easily use this command to select the rows that contain these words in your DataFrame. You need to use na equals False to ignore all rows containing missing values, otherwise the indexer won't work. Suppose you want to filter on a list of words, rather than just one. This is also easily done, by the using string contains function. You need to join the list of words with the or command, so you search on whether the text contains this or that word. From then on it is easy to create a flag for data that contain these words. By using the NumPy where function, you can simply create a new variable that flags one where the condition is met, and zero otherwise.\n",
    "\n",
    "![Alt text](image-24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Word search with dataframes\n",
    "In this exercise you're going to work with text data, containing emails from Enron employees. The Enron scandal is a famous fraud case. Enron employees covered up the bad financial position of the company, thereby keeping the stock price artificially high. Enron employees sold their own stock options, and when the truth came out, Enron investors were left with nothing. The goal is to find all emails that mention specific words, such as \"sell enron stock\".\n",
    "\n",
    "By using string operations on dataframes, you can easily sift through messy email data and create flags based on word-hits. The Enron email data has been put into a dataframe called df so let's search for suspicious terms. Feel free to explore df in the Console before getting started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find all cleaned emails that contain 'sell enron stock'\n",
    "mask = df['clean_content'].str.contains('sell enron stock', na=False)\n",
    "\n",
    "# Select the data from df using the mask\n",
    "print(df.loc[mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using list of terms\n",
    "Oftentimes you don't want to search on just one term. You probably can create a full \"fraud dictionary\" of terms that could potentially flag fraudulent clients and/or transactions. Fraud analysts often will have an idea what should be in such a dictionary. In this exercise you're going to flag a multitude of terms, and in the next exercise you'll create a new flag variable out of it. The 'flag' can be used either directly in a machine learning model as a feature, or as an additional filter on top of your machine learning model results. Let's first use a list of terms to filter our data on. The dataframe containing the cleaned emails is again available as df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of terms to search for\n",
    "searchfor = ['enron stock', 'sell stock', 'stock bonus', 'sell enron stock']\n",
    "\n",
    "# Filter cleaned emails on searchfor list and select from df \n",
    "filtered_emails = df.loc[df['clean_content'].str.contains('|'.join(searchfor), na=False)]\n",
    "print(filtered_emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a flag\n",
    "This time you are going to create an actual flag variable that gives a 1 when the emails get a hit on the search terms of interest, and 0 otherwise. This is the last step you need to make in order to actually use the text data content as a feature in a machine learning model, or as an actual flag on top of model results. You can continue working with the dataframe df containing the emails, and the searchfor list is the one defined in the last exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create flag variable where the emails match the searchfor terms\n",
    "df['flag'] = np.where((df['clean_content'].str.contains('|'.join(searchfor)) == True), 1, 0)\n",
    "\n",
    "# Count the values of the flag variable\n",
    "count = df['flag'].value_counts()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text mining to detect fraud\n",
    "\n",
    "2. Cleaning your text data\n",
    "\n",
    "Whenever you work with text data, be it for word search, topic modeling, sentiment analysis, or text style, you need to do some rigorous text cleaning in order to be able to work with the data. Here are four steps you must take, before working with the data further. First, you always need to split the text into sentences, and the sentences into words. **Transform everything into lowercase and remove punctuation. This is called tokenization.** Secondly, **you need to remove all stopwords as they mess up your data.** Luckily, there are dictionaries for this to help you do that. Thirdly, **you need to lemmatize words**. For example, this means changing words from ?third person into first person, **changing verbs in past and future tenses into present tenses.** This allows you to combine all words that point to the same thing. Lastly, **all verbs need to be stemmed,** such that they are reduced to their root form. For example, walking and walked are reduced to just their stem, walk.\n",
    "\n",
    "3. Go from this...\n",
    "\n",
    "When you take these four steps, it allows you to go from this type of data\n",
    "\n",
    "![Alt text](image-26.png)\n",
    "\n",
    "4. To this...\n",
    "\n",
    "to this nice, clean, structured list of words per dataframe row.\n",
    "\n",
    "![Alt text](image-25.png)\n",
    "\n",
    "5. Data preprocessing part 1\n",
    "\n",
    "Let's look at how to clean your data in practice. Following the four steps, you begin by tokenizing the text data into words. Tokenizers divide strings into lists of substrings. The standard nltk word tokenizer can be used to find the words and punctuation in a string. It splits the words on, for example, white space, and separates the punctuations out. You then use rstrip to remove all the whitespaces from the beginning and end of the strings, and, finally, you make sure all text is lower case, by replacing all letters with their lowercase counterpart, using regular expressions. You then clean the text further by removing stopwords and punctuation. You can use the tokenized text and get rid of all punctuation. Nltk has a stopwords list for the English language that you can use. Since every row consists of a list of strings, you need to create small loops that select the words you want to keep. I use join here to separate the words I want to keep with a space.\n",
    "\n",
    "![Alt text](image-27.png)\n",
    "\n",
    "6. Data preprocessing part 2\n",
    "\n",
    "The next step is to lemmatize the words, this can be easily done with the nltk WordNetLemmatizer. Again, I loop over the words and make sure they are joined together with a space between them. Stemming your verbs is equally simple, you can use the nltk PorterStemmer for this. After this work your text data should be nice and clean. It consists of lists of cleaned words for each row in your dataframe.\n",
    "\n",
    "![Alt text](image-28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Removing stopwords\n",
    "In the following exercises you're going to clean the Enron emails, in order to be able to use the data in a topic model. Text cleaning can be challenging, so you'll learn some steps to do this well. The dataframe containing the emails df is available. In a first step you need to define the list of stopwords and punctuations that are to be removed in the next exercise from the text data. Let's give it a try.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nltk packages and string \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Define stopwords to exclude\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update((\"to\",\"cc\",\"subject\",\"http\",\"from\",\"sent\", \"ect\", \"u\", \"fwd\", \"www\", \"com\"))\n",
    "\n",
    "# Define punctuations to exclude and lemmatizer\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text data\n",
    "Now that you've defined the stopwords and punctuations, let's use these to clean our enron emails in the dataframe df further. The lists containing stopwords and punctuations are available under stop and exclude There are a few more steps to take before you have cleaned data, such as \"lemmatization\" of words, and stemming the verbs. The verbs in the email data are already stemmed, and the lemmatization is already done for you in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the lemmatizer from nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Define word cleaning function\n",
    "def clean(text, stop):\n",
    "    text = text.rstrip()\n",
    "    stop_free = \" \".join([i for i in text.lower().split() if((i not in stop) and (not i.isdigit()))])\n",
    "    punc_free = ''.join(i for i in stop_free if i not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(i) for i in punc_free.split())      \n",
    "    return normalized\n",
    "\n",
    "# Clean the emails in df and print results\n",
    "text_clean=[]\n",
    "for text in df['clean_content']:\n",
    "    text_clean.append(clean(text, stop).split())    \n",
    "print(text_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have cleaned your data entirely with the necessary steps, including splitting the text into words, removing stopwords and punctuations, and lemmatizing your words. You are now ready to run a topic model on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling on fraud\n",
    "\n",
    "2. Topic modeling: discover hidden patterns in text data\n",
    "\n",
    "Topic modeling can be a powerful tool when searching for fraud in text data. Topic modeling allows you to discover abstract topics that occur in a collection of documents. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently. Topic modeling, therefore, tells us in a very efficient way what the text is about, based on the words it contains. Conceptually, it is similar to clustering, as it clusters words belonging to the same topic together. If you have text data of known fraud cases, it allows you to check what are the most common topics for those fraud cases, and use that to compare unknown cases. Without known labels, you can inspect which topics seem to point to fraudulent behavior and are interesting to further investigate.\n",
    "\n",
    "3. Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Latent Dirichlet Allocation, or LDA, is an example of topic model and is commonly used. It creates a \"topic per text item\" model and \"words per topic\" model, which are called Dirichlet distributions. Implementation of LDA is straightforward. First, you need to clean your data as described in the last video, and this is the most work. Then, you create a dictionary containing which words appear how often in all of the text, and also a corpus, containing, for each text line in your data, the count of words that appear.\n",
    "\n",
    "4. Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "The results you get from this model are twofold. First, you see how each word in your total data is associated with each topic. Second, you can also see how each text item in your data associates with topics, also in the form of probabilities. You can see this in the image here on the right. This image comes from a blogpost on DataCamp about LDA, which I encourage you to read if you want to learn more in detail.\n",
    "\n",
    "5. Bag of words: dictionary and corpus\n",
    "\n",
    "Let's talk about the implementation of an LDA model. You start by importing the corpora function from gensim. I use the dictionary function in corpora to create a dictionary from our text data, in this case, from the cleaned emails. The dictionary contains the number of times a word appears for each word. You then filter out words that appear in less than 5 emails and keep only the 50000 most frequent words, in a way of cleaning out the outlier noise of the text data. Last, you create a corpus that tells you, for each email, how many words it contains and how many times those words appear. You can use the doc2bow function for this. Doc2bow stands for document to bag of words. This function converts our text data into a bag-of-words format. That means, each row in our data is now a list of words with their associated word count.\n",
    "\n",
    "6. Latent Dirichlet Allocation (LDA) with gensim\n",
    "\n",
    "After cleaning the text data, and creating dictionary and corpus, you are now ready to run your LDA model. I use gensim again for this. You need to pass the corpus and dictionary into the LDA model. As with K-means, you need to pick the number of topics you want beforehand, even if you're not sure yet what the topics are. The LDA model calculated here, now contains the associated words for each topic, and the topic scores per email. You can obtain the top words from the three topics with the function print_topics. As you can see, after running the model, I print the three topics and the four top keywords associated with the topic, for a first interpretation of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
