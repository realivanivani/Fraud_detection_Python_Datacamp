{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review of classification methods\n",
    "\n",
    "2. What is classification?\n",
    "\n",
    "Classification is the problem of identifying to which class a new observation belongs, on the basis of a training set of data containing observations whose class is known. Classes are sometimes called **targets, labels or categories.** For example, spam detection in email service providers can be identified as a classification problem. This is a binary classification since there are only two classes as spam and not spam. **Fraud detection is a classification problem**, as we try to predict whether observations are fraudulent, yes or no. Lastly, assigning a diagnosis to a patient based on characteristics of a tumor, malignant or benign, is a classification problem. Classification problems normally have a categorical output like a yes or no, 1 or 0, True or False. In the case of fraud detection, the negative non-fraud class is the majority class, whereas the fraud cases are the minority class.\n",
    "\n",
    "3. Classification methods commonly used for fraud detection\n",
    "\n",
    "**Logistic regression is one of the most used machine learning algorithms for binary classification.** It is a simple algorithm that you can use as a performance baseline. It is easy to implement and it will do well enough in many tasks. **It can also be adjusted to work reasonably well on highly imbalanced data, which makes it quite useful for fraud detection.** I expect you to be familiar with this model, so I won't go into further details about how this model precisely works.\n",
    "\n",
    "4. Classification methods commonly used for fraud detection\n",
    "\n",
    "**Neural networks can also be used as classifiers for fraud detection.** They are capable of **fitting highly non-linear models** to our data. They tend to be slightly more complex to implement than most of the other classifiers we discuss in this course, so you'll not use this classifier in the exercises. Nonetheless, it's important to be aware that this is a model, suitable to use for fraud detection.\n",
    "\n",
    "5. Classification methods commonly used for fraud detection\n",
    "\n",
    "Decision trees and random forest and very commonly used in fraud detection. As you can see in the picture, decision trees give very transparent results, **that are easily interpreted by fraud analysts**. Nonetheless, **they are prone to overfit to your data.** Random forests are, therefore, a more robust option to use, as they construct a multitude of decision trees when training your model and outputting the class that is the mode or mean predicted class of all the individual trees.\n",
    "\n",
    "6. Decision trees and random forests\n",
    "\n",
    "To be more precise, a random forest consists of a collection of trees on a random subset of features. Final predictions are the combined results of those trees. **Random forests can handle complex data and are not prone to overfit.** They are interpretable by looking at feature importance, and can be adjusted to work well on highly imbalanced data. **The only drawback is that they can be computationally quite heavy to run.** Nonetheless, random forests are **very popular for fraud detection.** Throughout the exercises, you'll work on optimizing a random forest on our credit card fraud data.\n",
    "\n",
    "7. Random forests for fraud detection\n",
    "\n",
    "To refresh your memory, here is how to implement a simple random forest model using scikit-learn. First, you need to import the model from the ensemble package. Then, let's define the model here. The next step is to fit the model to your training set. Once that's done, you can obtain model predictions by running it on the test set. And lastly, you can obtain a measure of performance, let's take accuracy, by comparing your predictions to the actual labels under y_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Natural hit rate\n",
    "\n",
    "In this exercise, you'll again use credit card transaction data. The features and labels are similar to the data in the previous chapter, and the data is heavily imbalanced. We've given you features X and labels y to work with already, which are both numpy arrays.\n",
    "\n",
    "First you need to explore how prevalent fraud is in the dataset, to understand what the \"natural accuracy\" is, if we were to predict everything as non-fraud. It's is important to understand which level of \"accuracy\" you need to \"beat\" in order to get a better prediction than by doing nothing. In the following exercises, you'll create our first random forest classifier for fraud detection. That will serve as the \"baseline\" model that you're going to try to improve in the upcoming exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of observations from the length of y\n",
    "total_obs = y.size\n",
    "\n",
    "# Count the total number of non-fraudulent observations \n",
    "non_fraud = [i for i in y if i == 0]\n",
    "count_non_fraud = non_fraud.count(0)\n",
    "\n",
    "# Calculate the percentage of non fraud observations in the dataset\n",
    "percentage = (float(count_non_fraud)/float(total_obs)) * 100\n",
    "\n",
    "# Print the percentage: this is our \"natural accuracy\" by doing nothing\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier - part 1\n",
    "Let's now create a first random forest classifier for fraud detection. Hopefully you can do better than the baseline accuracy you've just calculated, which was roughly 96%. This model will serve as the \"baseline\" model that you're going to try to improve in the upcoming exercises. Let's start first with splitting the data into a test and training set, and defining the Random Forest model. The data available are features X and labels y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the random forest model from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Split your data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Define the model as the random forest\n",
    "model = RandomForestClassifier(random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain predictions from the test data \n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Print the accuracy performance metric\n",
    "print(accuracy_score(predicted, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest prevents overfitting most of the time, by creating random subsets of the features and building smaller trees using these subsets. Afterwards, it combines the subtrees of subsamples of features, so it does not tend to overfit to your entire feature set the way \"deep\" Decisions Trees do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation\n",
    "\n",
    "2. Accuracy isn't everything\n",
    "\n",
    "As you can see on these two images, accuracy is not a reliable performance metric when working with highly imbalanced data, as is the case in fraud detection. By doing nothing, aka predicting everything is the majority class, in the picture on the right, you often obtain a higher accuracy than by actually trying to build a predictive model, in the picture on the left. So let's discuss other performance metrics that are actually informative and reliable.\n",
    "\n",
    "3. False positives, false negatives, and actual fraud caught\n",
    "\n",
    "First of all, you need to understand the concepts of false positives, false negatives etc really well for fraud detection. Let's refresh that for a moment. The true positives and true negatives are the cases you are predicting correct, in our case, fraud and non-fraud. The images on the top left and bottom right are true negatives and true positives, respectively. The false negatives as seen in the bottom left, is predicting the person is not pregnant, but actually is. So these are the cases of fraud you are not catching with your model. The false positives in the top right are the cases that we predict to be pregnant, but aren't actually. These are \"false alarm\" cases, and can result in a burden of work whilst there actually is nothing going on. Depending on the business case, one might care more about false negatives than false positives, or vice versa. A credit card company might want to catch as much fraud as possible and reduce false negatives, as fraudulent transactions can be incredibly costly, whereas a false alarm just means someone's transaction is blocked. On the other hand, an insurance company can not handle many false alarms, as it means getting a team of investigators involved for each positive prediction.\n",
    "![Alt text](image.png)\n",
    "\n",
    "4. Precision-recall tradeoff\n",
    " \n",
    "The credit card company, therefore, wants to optimize for recall, whereas the insurance company cares more for precision. **Precision is the fraction of actual fraud cases out of all predicted fraud cases**, ie, the true positives relative to the true positives plus false positives, while vice-versa **recall is the fraction of predicted fraud cases out of all the actual fraud cases**, ie, the true positives relative to the true positives plus false negatives. Typically, **precision and recall are inversely related**. Basically as precision increases, recall falls and vice versa. You can plot the tradeoff between the two in the precision-recall curve, as seen here on the left. The F-score weighs both precision and recall into one measure, so if you want to use a performance metric that takes into account a **balance between precision and recall, F-score** is the one to use.\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "5. Obtaining performance metrics\n",
    "\n",
    "Obtaining precision and recall from scikit-learn is relatively straightforward. These are the packages you need. The average precision is calculated with the average_precision_score, which you need to run on the actual labels y_test and your predictions. The curve is obtained in a similar way, which you can then plot to look at the trade-off between the two.\n",
    "\n",
    "7. ROC curve to compare algorithms\n",
    "\n",
    "Another useful tool in the performance toolbox is the ROC curve. **ROC stands for receiver operating characteristic curve**, and is created by plotting the true positive rate against the false positive rate at various threshold settings. **The ROC curve is very useful for comparing performance of different algorithms for your fraud detection problem.** The \"area under the ROC curve\"-metric is easily obtained by getting the model probabilities like this, and then comparing those with the actual labels.\n",
    "![Alt text](image-2.png)\n",
    "\n",
    "8. Confusion matrix and classification report\n",
    "\n",
    "The confusion matrix and classification report are an absolute must have for fraud detection performance. You can obtain these from the scikit-learn metrics package. You need the model predictions for these, so not the probabilities. The classification report gives you precision, recall, and F1-score across the different labels. The confusion matrix plots the false negatives, false positives, etc for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Performance metrics for the RF model\n",
    "In the previous exercises you obtained an accuracy score for your random forest model. This time, we know accuracy can be misleading in the case of fraud detection. With highly imbalanced fraud data, the AUROC curve is a more reliable performance metric, used to compare different classifiers. Moreover, the classification report tells you about the precision and recall of your model, whilst the confusion matrix actually shows how many fraud cases you can predict correctly. So let's get these performance metrics.\n",
    "\n",
    "You'll continue working on the same random forest model from the previous exercise. Your model, defined as model = RandomForestClassifier(random_state=5) has been fitted to your training data already, and X_train, y_train, X_test, y_test are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the packages to get the different performance metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Obtain the predictions from our random forest model \n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Predict probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# Print the ROC curve, classification report and confusion matrix\n",
    "print(roc_auc_score(y_test, probs[:,1]))\n",
    "print(classification_report(y_test, predicted))\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the Precision Recall Curve\n",
    "You can also plot a Precision-Recall curve, to investigate the trade-off between the two in your model. In this curve Precision and Recall are inversely related; as Precision increases, Recall falls and vice-versa. A balance between these two needs to be achieved in your model, otherwise you might end up with many false positives, or not enough actual fraud cases caught. To achieve this and to compare performance, the precision-recall curves come in handy.\n",
    "\n",
    "Your Random Forest Classifier is available as model, and the predictions as predicted. You can simply obtain the average precision score and the PR curve from the sklearn package. The function plot_pr_curve() plots the results for you. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average precision and the PR curve\n",
    "average_precision = average_precision_score(y_test, predicted)\n",
    "\n",
    "# Obtain precision and recall \n",
    "precision, recall, _ = precision_recall_curve(y_test, predicted)\n",
    "\n",
    "# Plot the recall precision tradeoff\n",
    "plot_pr_curve(recall, precision, average_precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve plots the true positives vs. false positives , for a classifier, as its discrimination threshold is varied. Since, a random method describes a horizontal curve through the unit interval, it has an AUC of 0.5. Minimally, classifiers should perform better than this, and the extent to which they score higher than one another (meaning the area under the ROC curve is larger), they have better expected performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting your algorithm weights\n",
    "\n",
    "Balanced weights\n",
    "\n",
    "When training a model for fraud detection, you want to try different options and setting to get the best recall-precision tradeoff possible. In scikit-learn there are two simple options to tweak your model **for heavily imbalanced fraud data**. There is the **balanced mode**, and **balanced_subsample mode**, that you can assign to the weight argument when defining the model. The balanced mode uses the values of y to **automatically adjust weights inversely proportional to class frequencies in the input data**. The balanced_subsample mode is the same as the balanced option, except that weights are calculated again at each iteration of growing a tree in the random forest. This latter option is therefore only applicable for the random forest model. The balanced option is however also available for many other classifiers, for example the logistic regression has the option, as well as the SVM model.\n",
    "![Alt text](image-5.png)\n",
    "\n",
    "3. Hyperparameter tuning for fraud detection\n",
    "\n",
    "The weight option also takes a manual input. This allows you to adjust weights not based on the value counts relative to sample, but to whatever ratio you like. So if you just want to upsample your minority class slightly, then this is a good option. All the classifiers that have the weight option available should have this manual setting also. Moreover, the random forest takes many other options you can use to optimize the model; you call this **hyperparameter tuning**. You can, for example, change the shape and size of the trees in the random forest by adjusting leaf size and tree depth. One of the most important settings are the number of trees in the forest, called number of estimators, and the number of features considered for splitting at each leaf node, indicated by max_features. Moreover, you can change the way the data is split at each node, as the default is to split on the gini coefficient. Without going into too much detail now, I encourage you to research the different options when working with this model.\n",
    "\n",
    "4. Using GridSearchCV\n",
    "\n",
    "A smarter way of hyperparameter tuning your model is to use **GridSearchCV**. You should have come across this in the course on Supervised Learning. Let's import the package first. GridSearchCV evaluates all combinations of parameters we define in the parameter grid. This is an example of a parameter grid specifically for a random forest model. Let's define the machine learning model we'll use. And now, let's put it into a grid search. You pass in the model, the parameter grid, and we'll tell it how often to cross-validate. Most importantly, **you need to define a scoring metric to evaluate the models on**. This is incredibly important in fraud detection. The default option here would be accuracy, so if you don't define this, your models are ranked based on accuracy, which you already know is useless.**You therefore need to pass the option precision, recall, or F1** here. Let's go with F1 for this example.\n",
    "![Alt text](image-4.png)\n",
    "\n",
    "5. Finding the best model with GridSearchCV\n",
    "\n",
    "Once you have fitted your GridSearchCV and model to the data, you can obtain the parameters belonging to the optimal model by using the best parameters function. Mind you, **GridSearchCV is computationally very heavy to run**. Depending on the size of your data, and number of parameters in the grid, this can take up to many hours to complete, so **make sure to save the results**. You can easily get the results for the best_estimator that gives the best_score; these results are all stored. The best score is the mean cross-validated score of the best_estimator, which of course also depends on the scoring option you gave earlier. As you chose F1 before, you'll get the best F1 sore here.\n",
    "![Alt text](image-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Model adjustments\n",
    "A simple way to adjust the random forest model to deal with highly imbalanced fraud data, is to use the class_weights option when defining your sklearn model. However, as you will see, it is a bit of a blunt force mechanism and might not work for your very special case.\n",
    "\n",
    "In this exercise you'll explore the weight = \"balanced_subsample\" mode the Random Forest model from the earlier exercise. You already have split your data in a training and test set, i.e X_train, X_test, y_train, y_test are available. The metrics function have already been imported."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model with balanced subsample\n",
    "model = RandomForestClassifier(class_weight='balanced_subsample', random_state=5)\n",
    "\n",
    "# Fit your training model to your training set\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain the predicted values and probabilities from the model \n",
    "predicted = model.predict(X_test)\n",
    "probs = model.predict_proba(X_test)\n",
    "\n",
    "# Print the roc_auc_score, the classification report and confusion matrix\n",
    "print(roc_auc_score(y_test, probs[:,1]))\n",
    "print(classification_report(y_test, predicted))\n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the model results don't improve drastically. We now have 3 less false positives, but now 19 in stead of 18 false negatives, i.e. cases of fraud we are not catching. If we mostly care about catching fraud, and not so much about the false positives, this does actually not improve our model at all, albeit a simple option to try. In the next exercises you'll see how to more smartly tweak your model to focus on reducing false negatives and catch more fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting your Random Forest to fraud detection\n",
    "In this exercise you're going to dive into the options for the random forest classifier, as we'll assign weights and tweak the shape of the decision trees in the forest. You'll define weights manually, to be able to off-set that imbalance slightly. In our case we have 300 fraud to 7000 non-fraud cases, so by setting the weight ratio to 1:12, we get to a 1/3 fraud to 2/3 non-fraud ratio, which is good enough for training the model on.\n",
    "\n",
    "The data in this exercise has already been split into training and test set, so you just need to focus on defining your model. You can then use the function get_model_results() as a short cut. This function fits the model to your training data, predicts and obtains performance metrics similar to the steps you did in the previous exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the model options\n",
    "model = RandomForestClassifier(bootstrap=True, class_weight={0:1, 1:12}, criterion='entropy',\n",
    "\t\t\t\n",
    "\t\t\t# Change depth of model\n",
    "            max_depth=10,\n",
    "\t\t\n",
    "\t\t\t# Change the number of samples in leaf nodes\n",
    "            min_samples_leaf=10, \n",
    "\n",
    "\t\t\t# Change the number of trees to use\n",
    "            n_estimators=20, n_jobs=-1, random_state=5)\n",
    "\n",
    "# Run the function get_model_results\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You can see by smartly defining more options in the model, you can obtain better predictions. You have effectively reduced the number of false negatives, i.e. you are catching more cases of fraud, whilst keeping the number of false positives low. In this exercise you've manually changed the options of the model. There is a smarter way of doing it, by using GridSearchCV, which you'll see in the next exercise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearchCV to find optimal parameters\n",
    "In this exercise you're going to tweak our model in a less \"random\" way, but use GridSearchCV to do the work for you.\n",
    "\n",
    "With GridSearchCV you can define which performance metric to score the options on. Since for fraud detection we are mostly interested in catching as many fraud cases as possible, you can optimize your model settings to get the best possible Recall score. If you also cared about reducing the number of false positives, you could optimize on F1-score, this gives you that nice Precision-Recall trade-off.\n",
    "\n",
    "GridSearchCV has already been imported from sklearn.model_selection, so let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter sets to test\n",
    "param_grid = {'n_estimators': [1, 30], 'max_features': ['auto', 'log2'],  'max_depth': [4, 8], 'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the model to use\n",
    "model = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "\n",
    "# Fit the model to our training data and obtain best parameters\n",
    "CV_model.fit(X_train, y_train)\n",
    "CV_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model results using GridSearchCV\n",
    "You discovered that the best parameters for your model are that the split criterion should be set to 'gini', the number of estimators (trees) should be 30, the maximum depth of the model should be 8 and the maximum features should be set to \"log2\".\n",
    "\n",
    "Let's give this a try and see how well our model performs. You can use the get_model_results() function again to save time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the optimal parameters in the model\n",
    "model = RandomForestClassifier(class_weight={0:1,1:12}, criterion='gini',\n",
    "            max_depth=8, max_features='log2',  min_samples_leaf=10, n_estimators=30, n_jobs=-1, random_state=5)\n",
    "\n",
    "# Get results from your model\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've managed to improve your model even further. The number of false negatives has now been slightly reduced even further, which means we are catching more cases of fraud. However, you see that the number of false positives actually went up. That is that Precision-Recall trade-off in action. To decide which final model is best, you need to take into account how bad it is not to catch fraudsters, versus how many false positives the fraud analytics team can deal with. Ultimately, this final decision should be made by you and the fraud team together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensamble methods\n",
    "\n",
    "2. What are ensemble methods: bagging versus stacking\n",
    "\n",
    "Ensemble methods are techniques that create multiple machine learning models and then combine them to produce a final result. **Ensemble methods usually produce more accurate predictions than a single model would**. In fact, you've already worked with an ensemble method during the exercises. The random forest classifier is an ensemble of decision trees, and is described as a **bootstrap aggregation**, or **bagging ensemble method**. In a random forest, you train models on random subsamples of your data and aggregate the results by taking the average prediction of all of the trees.\n",
    "\n",
    "3. Stacking ensemble methods\n",
    "\n",
    "In this picture, you see a stacking ensemble method. In this case, multiple models are combined via a voting rule on the model outcome. The base level models are each trained based on the complete training set. So, unlike with the bagging method, you do not train your models on a subsample. In the stacking ensemble method, you **can combine algorithms of different types**. We'll practice this in the exercises.\n",
    "![Alt text](image-6.png)\n",
    "\n",
    "4. Why use ensemble methods for fraud detection\n",
    "\n",
    "The goal of any machine learning problem is to find a single model that will best predict the wanted outcome. Rather than making one model, and hoping this model is the best or most accurate predictor, you can make use of ensemble methods. Ensemble methods take a myriad of models into account, and average those models to produce one final model. This ensures that your **predictions are robust and less likely to be the result of overfitting**. Moreover, ensemble methods can improve overall performance of fraud detection, especially combining models with different recall and precision scores. They have therefore been a winning formula at many Kaggle competitions recently.\n",
    "\n",
    "5. Voting classifier\n",
    "\n",
    "The **voting classifier** available in scikit-learn is an easy way to implement an ensemble model. You start by importing the voting classifier, available from the ensemble methods package. Let's define three models to use in our ensemble model, in this case let's use a random forest, a logistic regression, and a Naive Bayes model. The next step is to combine these three into the ensemble model like this, and assign a rule to combine the model results. In this case, let's use a **hard voting rule**. That option uses the predicted class labels and takes the **majority vote**. The other option is **soft voting**. This rule takes the **average probability** by combining the predicted probabilities of the individual models. You can then simply use the ensemble_model as you would any other machine learning model, ie you can fit and use the model to predict classes. Last thing to mention is that you can also assign weights to the model predictions in the ensemble, which can be useful, for example, when you know one model outperforms the others significantly.\n",
    "![Alt text](image-7.png)\n",
    "6. Reliable labels for fraud detection\n",
    "\n",
    "In this chapter, you have seen how to detect fraud when there are labels to train a model on. However, in **real life, it is unlikely that you will have truly unbiased reliable labels for you model**. For example, in credit card fraud you often will have reliable labels, in which case you want to use these methods you've just learned. However, in most other cases, you will need to rely on unsupervised learning techniques to detect fraud. You will learn how to do this in the upcoming chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Logistic Regression\n",
    "In this last lesson you'll combine three algorithms into one model with the VotingClassifier. This allows us to benefit from the different aspects from all models, and hopefully improve overall performance and detect more fraud. The first model, the Logistic Regression, has a slightly higher recall score than our optimal Random Forest model, but gives a lot more false positives. You'll also add a Decision Tree with balanced weights to it. The data is already split into a training and test set, i.e. X_train, y_train, X_test, y_test are available.\n",
    "\n",
    "In order to understand how the Voting Classifier can potentially improve your original model, you should check the standalone results of the Logistic Regression model first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Logistic Regression model with weights\n",
    "model = LogisticRegression(class_weight={0:1, 1:15}, random_state=5)\n",
    "\n",
    "# Get the model results\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting Classifier\n",
    "Let's now combine three machine learning models into one, to improve our Random Forest fraud detection model from before. You'll combine our usual Random Forest model, with the Logistic Regression from the previous exercise, with a simple Decision Tree. You can use the short cut get_model_results() to see the immediate result of the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Define the three classifiers to use in the ensemble\n",
    "clf1 = LogisticRegression(class_weight={0:1, 1:15}, random_state=5)\n",
    "clf2 = RandomForestClassifier(class_weight={0:1, 1:12}, criterion='gini', max_depth=8, max_features='log2',\n",
    "            min_samples_leaf=10, n_estimators=30, n_jobs=-1, random_state=5)\n",
    "clf3 = DecisionTreeClassifier(random_state=5, class_weight=\"balanced\")\n",
    "\n",
    "# Combine the classifiers in the ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('dt', clf3)], voting='hard')\n",
    "\n",
    "# Get the results \n",
    "get_model_results(X_train, y_train, X_test, y_test, ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that by combining the classifiers, you can take the best of multiple models. You've increased the cases of fraud you are catching from 76 to 78, and you only have 5 extra false positives in return. If you do care about catching as many fraud cases as you can, whilst keeping the false positives low, this is a pretty good trade-off. The Logistic Regression as a standalone was quite bad in terms of false positives, and the Random Forest was worse in terms of false negatives. By combining these together you indeed managed to improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust weights within the Voting Classifier\n",
    "You've just seen that the Voting Classifier allows you to improve your fraud detection performance, by combining good aspects from multiple models. Now let's try to adjust the weights we give to these models. By increasing or decreasing weights you can play with how much emphasis you give to a particular model relative to the rest. This comes in handy when a certain model has overall better performance than the rest, but you still want to combine aspects of the others to further improve your results.\n",
    "\n",
    "For this exercise the data is already split into a training and test set, and clf1, clf2 and clf3 are available and defined as before, i.e. they are the Logistic Regression, the Random Forest model and the Decision Tree respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ensemble model\n",
    "ensemble_model = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], \n",
    "    voting='soft', \n",
    "    weights=[1, 4, 1], \n",
    "    flatten_transform=True)\n",
    "\n",
    "# Get results \n",
    "get_model_results(X_train, y_train, X_test, y_test, ensemble_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight option allows you to play with the individual models to get the best final mix for your fraud detection model. Now that you have finalized fraud detection with supervised learning, let's have a look at how fraud detection can be done when you don't have any labels to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
