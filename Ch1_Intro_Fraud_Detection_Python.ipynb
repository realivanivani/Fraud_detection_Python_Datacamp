{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fadd392",
   "metadata": {},
   "source": [
    "### Introduction to fraud detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec2ed17",
   "metadata": {},
   "source": [
    "1. Introduction to fraud detection\n",
    "\n",
    "Welcome to this DataCamp course on fraud detection. To benefit from this course, you should be comfortable with manipulating dataframes, visualizing data, and have foundations in supervised and unsupervised learning.\n",
    "\n",
    "2. Meet your instructor\n",
    "\n",
    "I'll be your teacher throughout this course, and my name is Charlotte. Did you know that a typical organization loses 5 percent of its revenue to fraud each year? In fact, it is estimated that fraud is costing the UK economy 73 billion British pounds each year. As such, you can say, fraud poses a serious problem to almost all companies. This course teaches you how you can tackle fraud as a data scientist, and thereby make a tangible impact on your company.\n",
    "\n",
    "3. What is fraud?\n",
    "\n",
    "Fraudulent behavior can be found in many different areas. Credit card fraud is perhaps the most famous example, and also in the insurance industry, fraud is a well-known issue. But it is much more broadly present than that. For example even all e-commerce businesses need to continuously assess whether client transactions on their website are legit. Detecting fraud is typically challenging because of these four characteristics of fraud described here. First of all, fraud cases are in a minority, sometimes only one-hundredth percent of a companies' transactions are fraudulent. Fraudsters will also try their best to \"blend\" in and conceal their activities. Moreover, fraudsters will find new methods to avoid getting caught, and change their behavior over time. Lastly, fraudsters oftentimes work together and organize their activities in a network, making it harder to detect. It can be that multiple client accounts are involved around one fraud case. Let's illustrate this with an example.\n",
    "\n",
    "4. Fraud detection is challenging\n",
    "\n",
    "Have you ever played \"Where is Waldo\" or \"Find the odd one out\"? Like in the game, in fraud detection you'll need to train an algorithm to pick a well concealed observation out of many normal observations. Can you find the odd one out here?\n",
    "\n",
    "5. Fraud detection is challenging\n",
    "\n",
    "Here it is. It looks like the other clovers, but it deviates slightly. That one was easy, but it does get much harder when we're working with numbers, so have a look at this one.\n",
    "\n",
    "6. Fraud detection is challenging\n",
    "\n",
    "This is much more like in real life, we'll need to find a fraud case based on numbers. The case we're looking for is well concealed and only one of these is odd. Can you find it?\n",
    "\n",
    "7. Fraud detection is challenging\n",
    "\n",
    "Here it is, 26. It's the only number in this set that's not divisible by 4. This illustrates a typical fraud detection problem really well: based on data, you'll need to train an algorithm to find the odd one out among many normal observations.\n",
    "\n",
    "8. How companies deal with fraud\n",
    "\n",
    "As a data scientist working on fraud analytics, you'll often be asked to improve existing fraud detection systems. You'll maybe find that the company already uses a rules based system to filter out strange cases. Or that the fraud analytics team checks the news for suspicious names, or keeps track of external hit lists from the police to reference check against the client base. All these existing methods can be useful for your machine learning model, as you can use them as inputs in your analysis. But do be mindful when using labels that come out of existing rules based systems; you should always ask yourself whether the labels are reliable as they might not catch all fraudulent cases.\n",
    "\n",
    "9. Let's have a look at some data\n",
    "\n",
    "In this chapter we'll explore a dataset on credit card transactions. We have 29 features available, and a Class variable, containing information about whether the transaction is fraudulent or not. We have data on 5050 transactions in total. This should be enough for training our first algorithm on.\n",
    "\n",
    "10. Let's practice!\n",
    "\n",
    "Now let's have a look at this credit card data in more detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3249065b",
   "metadata": {},
   "source": [
    "#### Checking the fraud to non-fraud ratio\n",
    "\n",
    "In this chapter, you will work on creditcard_sampledata.csv, a dataset containing credit card transactions data. Fraud occurrences are fortunately an extreme minority in these transactions.\n",
    "\n",
    "However, Machine Learning algorithms usually work best when the different classes contained in the dataset are more or less equally present. If there are few cases of fraud, then there's little data to learn how to identify them. This is known as class imbalance, and it's one of the main challenges of fraud detection.\n",
    "\n",
    "Let's explore this dataset, and observe this class imbalance problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and read csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"creditcard_data.csv\")\n",
    "\n",
    "# Explore the features available in your dataframe\n",
    "print(df.info())\n",
    "\n",
    "# Count the occurrences of fraud and no fraud and print them\n",
    "occ = df['Class'].value_counts()\n",
    "print(occ)\n",
    "\n",
    "# Print the ratio of fraud cases\n",
    "print(occ / df.index.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48adc8f",
   "metadata": {},
   "source": [
    "#### Plotting your data\n",
    "\n",
    "From the previous exercise we know that the ratio of fraud to non-fraud observations is very low. You can do something about that, for example by re-sampling our data, which is explained in the next video.\n",
    "\n",
    "In this exercise, you'll look at the data and visualize the fraud to non-fraud ratio. It is always a good starting point in your fraud analysis, to look at your data first, before you make any changes to it.\n",
    "\n",
    "Moreover, when talking to your colleagues, a picture often makes it very clear that we're dealing with heavily imbalanced data. Let's create a plot to visualize the ratio fraud to non-fraud data points on the dataset df.\n",
    "\n",
    "The function prep_data() is already loaded in your workspace, as well as matplotlib.pyplot as plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a1ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a scatter plot of our data and labels\n",
    "def plot_data(X, y):\n",
    "\tplt.scatter(X[y == 0, 0], X[y == 0, 1], label=\"Class #0\", alpha=0.5, linewidth=0.15)\n",
    "\tplt.scatter(X[y == 1, 0], X[y == 1, 1], label=\"Class #1\", alpha=0.5, linewidth=0.15, c='r')\n",
    "\tplt.legend()\n",
    "\treturn plt.show()\n",
    "\n",
    "# Create X and y from the prep_data function \n",
    "X, y = prep_data(df)\n",
    "\n",
    "# Plot our data by running our plot data function on X and y\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0085be",
   "metadata": {},
   "source": [
    "### Fraud Detection in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a0ce7",
   "metadata": {},
   "source": [
    "1. Increasing successful detections using data resampling\n",
    "\n",
    "In this video, we're going to talk about data resampling methods. They help us to better train our models to recognize fraud cases when there are only very few cases of fraud.\n",
    "\n",
    "2. Undersampling\n",
    "\n",
    "The most straightforward way to adjust the imbalance of your data, is to undersample the majority class, aka non-fraud cases, or oversample the minority class, aka the fraud cases. With undersampling, you take random draws from your non-fraud observations, to match the amount of fraud observations as seen on the picture.\n",
    "\n",
    "3. Oversampling\n",
    "\n",
    "With oversampling, you take random draws from the fraud cases, and copy those observations to increase the amount of fraud samples you have in your data. Both methods lead to having a perfect balance between fraud and non-fraud data. But there are drawbacks. With random undersampling, you are effectively throwing away a lot of data and information. With oversampling, you are simply copying data, and therefore are training your model on a lot of duplicates. Let's see how you can implement these methods in practice.\n",
    "\n",
    "4. Oversampling in Python\n",
    "\n",
    "You can implement resampling methods using Python's imbalanced learn module. It is compatible with scikit-learn and allows you to implement these methods in just two lines of code. As you can see here, I import the package and take the Random Oversampler and assign it to method. I simply fit the method onto my original feature set X, and labels y, to obtained a resampled feature set X, and resampled y. I plot the datasets here side by side, such that you can see the effect of my resampling method. The darker blue color of the data points reflect that there are more identical data points now.\n",
    "\n",
    "5. Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "The Synthetic Minority Oversampling Technique, or SMOTE, is another way of adjusting the imbalance by oversampling your minority observations, aka your fraud cases. But with SMOTE, we're not just copying the minority class. Instead, as you see in this picture, SMOTE uses characteristics of nearest neighbors of fraud cases to create new synthetic fraud cases, and thereby avoids duplicating observations.\n",
    "\n",
    "1 https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
    "6. Which resampling method to use?\n",
    "\n",
    "You might wonder which one of these methods is the best? Well, it depends very much on the situation. If you have very large amounts of data, and also many fraud cases, you might find it computationally easier to undersample, rather than to increase data even more. But in most cases, throwing away data is not desirable. When it comes to oversampling, SMOTE is more sophisticated as it does not duplicate data. But this only works well if your fraud cases are quite similar to each other. If fraud is spread out over your data and not very distinct, using nearest neighbors to create more fraud cases introduces a bit of noise in the data, as the nearest neighbors might not necessarily be fraud cases.\n",
    "\n",
    "7. When to use resampling methods\n",
    "\n",
    "One thing to keep in mind when using resampling methods, is to only resample on your training set. Your goal is to better train your model by giving it balanced amounts of data. Your goal is not to predict your synthetic samples. Always make sure your test data is free of duplicates or synthetic data, such that you can test your model on real data only. The way to do this, is to first split the data into a train and test set, as you can see here. I then resample the training set only. I fit my model into the resampled training data, and lastly, I obtain my performance metrics by looking at my original, not resampled, test data. These steps should look familiar to you.\n",
    "\n",
    "8. Let's practice!\n",
    "\n",
    "So, let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a6eeae",
   "metadata": {},
   "source": [
    "#### Applying SMOTE\n",
    "\n",
    "In this exercise, you're going to re-balance our data using the Synthetic Minority Over-sampling Technique (SMOTE). Unlike ROS, SMOTE does not create exact copies of observations, but creates new, synthetic, samples that are quite similar to the existing observations in the minority class. SMOTE is therefore slightly more sophisticated than just copying observations, so let's apply SMOTE to our credit card data. The dataset df is available and the packages you need for SMOTE are imported. In the following exercise, you'll visualize the result and compare it to the original data, such that you can see the effect of applying SMOTE very clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400de898",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Run the prep_data function\n",
    "X, y = prep_data(df)\n",
    "\n",
    "# Define the resampling method\n",
    "method = SMOTE()\n",
    "\n",
    "# Create the resampled feature set\n",
    "X_resampled, y_resampled = method.fit_resample(X, y)\n",
    "\n",
    "# Plot the resampled data\n",
    "plot_data(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de689318",
   "metadata": {},
   "source": [
    "Compare SMOTE to original data\n",
    "\n",
    "In the last exercise, you saw that using SMOTE suddenly gives us more observations of the minority class. Let's compare those results to our original data, to get a good feeling for what has actually happened. Let's have a look at the value counts again of our old and new data, and let's plot the two scatter plots of the data side by side. You'll use the pre-defined function compare_plot() for that that, which takes the following arguments: X, y, X_resampled, y_resampled, method=''. The function plots your original data in a scatter plot, along with the resampled side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value_counts on the original labels y\n",
    "print(pd.value_counts(pd.Series(y)))\n",
    "\n",
    "# Print the value_counts\n",
    "print(pd.value_counts(pd.Series(y_resampled)))\n",
    "\n",
    "# Run compare_plot\n",
    "compare_plot(X, y, X_resampled, y_resampled, method='SMOTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e68c347",
   "metadata": {},
   "source": [
    "### Fraud detection algorithms in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5fbd4",
   "metadata": {},
   "source": [
    "1. Fraud detection algorithms in action\n",
    "\n",
    "This video is about traditional fraud detection methods versus machine learning models. As a data scientist, you'll often be asked to defend your method of choice, so it is important to understand the intricacies of both methods. You'll also get a refresher on machine learning models to help you with the exercises.\n",
    "\n",
    "2. Traditional fraud detection with rules based systems\n",
    "\n",
    "Traditionally, fraud analysts use rules based systems for detection of fraud. For example in the case of credit cards, the analysts might create rules based on a location and block transactions from risky zip codes. They might also create rules to block transactions from cards used too frequently, for example in the last 30 minutes. Some of these rules can be highly efficient at catching fraud, whilst others are not and results in false alarm too often.\n",
    "\n",
    "3. Drawbacks of using rules based systems\n",
    "\n",
    "A major limitation of rules based systems, is that the thresholds per rule are fixed, and those do not adapt as fraudulent behavior changes over time. Also, it's very difficult to determine what the right threshold should be. Second, with a rule, you'll get a yes or no outcome, unlike with machine learning where you can get a probability value. With probabilities, you can much better fine tune the outcomes to the amount of cases you want to inspect as a fraud team. Effectively, with a machine learning model, you can easily determine how many false positives and false negatives are acceptable, and with rules that's much harder. Rules based system also cannot capture the interaction of features like machine learning models can. So, for example, suppose the size of a transaction only matters in combination with the frequency, for determining fraudulent transactions. A rules based systems cannot really deal with that.\n",
    "\n",
    "4. Why use machine learning for fraud detection?\n",
    "\n",
    "Machine learning models don't have these limitations. They will adapt to new data, and therefore can capture new fraudulent behavior. You are able to capture interactions between features, and can work with probabilities rather than yes/no answers. Machine learning models therefore typically have a better performance in fraud detection. However, machine learning models are not always the holy grail. Some simple rules might prove to be quite capable of catching fraud. You therefore want to explore whether you can combine models with rules, to improve overall performance.\n",
    "\n",
    "5. Refresher on machine learning models\n",
    "\n",
    "Because you'll be working with machine learning models in the exercises, here's a quick refresher about how to define one with scikit-learn. First, you always want to start with splitting your data into a train and a test set. The second step is to define which model you want to use, and define its parameters. Let's take a very simple linear model, without defining any parameters. You then continue by fitting your model onto your training data, you want to pass X_train and y_train into the model. Your model has now been trained and you can obtain predictions, by running the model-dot-predict() function onto X_test. The last step is to compare your predictions from the model, with the true values by combining the y_predicted with y_test in a test metric. Here, we obtain an R-squared score for our lineal model. We'll practice this once again in the exercises on a different model.\n",
    "\n",
    "6. What you'll be doing in the upcoming chapters\n",
    "\n",
    "In the following chapter, you'll learn how to adapt classification models to effectively detect fraud cases. In chapter 3, you'll explore the situation where there are no reliable labels, and you need to flag potential fraudsters by clustering your data. Lastly, in chapter 4, you'll learn how to further improve your models by analyzing text data and applying topic modeling to further detect fraud.\n",
    "\n",
    "7. Let's practice!\n",
    "\n",
    "So, let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576343b",
   "metadata": {},
   "source": [
    "#### Applying SMOTE\n",
    "\n",
    "In this exercise, you're going to re-balance our data using the Synthetic Minority Over-sampling Technique (SMOTE). Unlike ROS, SMOTE does not create exact copies of observations, but creates new, synthetic, samples that are quite similar to the existing observations in the minority class. SMOTE is therefore slightly more sophisticated than just copying observations, so let's apply SMOTE to our credit card data. The dataset df is available and the packages you need for SMOTE are imported. In the following exercise, you'll visualize the result and compare it to the original data, such that you can see the effect of applying SMOTE very clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb392ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Run the prep_data function\n",
    "X, y = prep_data(df)\n",
    "\n",
    "# Define the resampling method\n",
    "method = SMOTE()\n",
    "\n",
    "# Create the resampled feature set\n",
    "X_resampled, y_resampled = method.fit_resample(X, y)\n",
    "\n",
    "# Plot the resampled data\n",
    "plot_data(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82afab",
   "metadata": {},
   "source": [
    "#### Compare SMOTE to original data\n",
    "\n",
    "In the last exercise, you saw that using SMOTE suddenly gives us more observations of the minority class. Let's compare those results to our original data, to get a good feeling for what has actually happened. Let's have a look at the value counts again of our old and new data, and let's plot the two scatter plots of the data side by side. You'll use the pre-defined function compare_plot() for that that, which takes the following arguments: X, y, X_resampled, y_resampled, method=''. The function plots your original data in a scatter plot, along with the resampled side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value_counts on the original labels y\n",
    "print(pd.value_counts(pd.Series(y)))\n",
    "\n",
    "# Print the value_counts\n",
    "print(pd.value_counts(pd.Series(y_resampled)))\n",
    "\n",
    "# Run compare_plot\n",
    "compare_plot(X, y, X_resampled, y_resampled, method='SMOTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7713b6d7",
   "metadata": {},
   "source": [
    "### Fraud detection algorithms in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c08b117",
   "metadata": {},
   "source": [
    "1. Fraud detection algorithms in action\n",
    "\n",
    "This video is about traditional fraud detection methods versus machine learning models. As a data scientist, you'll often be asked to defend your method of choice, so it is important to understand the intricacies of both methods. You'll also get a refresher on machine learning models to help you with the exercises.\n",
    "\n",
    "2. Traditional fraud detection with rules based systems\n",
    "\n",
    "Traditionally, fraud analysts use rules based systems for detection of fraud. For example in the case of credit cards, the analysts might create rules based on a location and block transactions from risky zip codes. They might also create rules to block transactions from cards used too frequently, for example in the last 30 minutes. Some of these rules can be highly efficient at catching fraud, whilst others are not and results in false alarm too often.\n",
    "\n",
    "3. Drawbacks of using rules based systems\n",
    "\n",
    "A major limitation of rules based systems, is that the thresholds per rule are fixed, and those do not adapt as fraudulent behavior changes over time. Also, it's very difficult to determine what the right threshold should be. Second, with a rule, you'll get a yes or no outcome, unlike with machine learning where you can get a probability value. With probabilities, you can much better fine tune the outcomes to the amount of cases you want to inspect as a fraud team. Effectively, with a machine learning model, you can easily determine how many false positives and false negatives are acceptable, and with rules that's much harder. Rules based system also cannot capture the interaction of features like machine learning models can. So, for example, suppose the size of a transaction only matters in combination with the frequency, for determining fraudulent transactions. A rules based systems cannot really deal with that.\n",
    "\n",
    "4. Why use machine learning for fraud detection?\n",
    "\n",
    "Machine learning models don't have these limitations. They will adapt to new data, and therefore can capture new fraudulent behavior. You are able to capture interactions between features, and can work with probabilities rather than yes/no answers. Machine learning models therefore typically have a better performance in fraud detection. However, machine learning models are not always the holy grail. Some simple rules might prove to be quite capable of catching fraud. You therefore want to explore whether you can combine models with rules, to improve overall performance.\n",
    "\n",
    "5. Refresher on machine learning models\n",
    "\n",
    "Because you'll be working with machine learning models in the exercises, here's a quick refresher about how to define one with scikit-learn. First, you always want to start with splitting your data into a train and a test set. The second step is to define which model you want to use, and define its parameters. Let's take a very simple linear model, without defining any parameters. You then continue by fitting your model onto your training data, you want to pass X_train and y_train into the model. Your model has now been trained and you can obtain predictions, by running the model-dot-predict() function onto X_test. The last step is to compare your predictions from the model, with the true values by combining the y_predicted with y_test in a test metric. Here, we obtain an R-squared score for our lineal model. We'll practice this once again in the exercises on a different model.\n",
    "\n",
    "6. What you'll be doing in the upcoming chapters\n",
    "\n",
    "In the following chapter, you'll learn how to adapt classification models to effectively detect fraud cases. In chapter 3, you'll explore the situation where there are no reliable labels, and you need to flag potential fraudsters by clustering your data. Lastly, in chapter 4, you'll learn how to further improve your models by analyzing text data and applying topic modeling to further detect fraud.\n",
    "\n",
    "7. Let's practice!\n",
    "\n",
    "So, let's practice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2b5183",
   "metadata": {},
   "source": [
    "#### Exploring the traditional way to catch fraud\n",
    "\n",
    "In this exercise you're going to try finding fraud cases in our credit card dataset the \"old way\". First you'll define threshold values using common statistics, to split fraud and non-fraud. Then, use those thresholds on your features to detect fraud. This is common practice within fraud analytics teams.\n",
    "\n",
    "Statistical thresholds are often determined by looking at the mean values of observations. Let's start this exercise by checking whether feature means differ between fraud and non-fraud cases. Then, you'll use that information to create common sense thresholds. Finally, you'll check how well this performs in fraud detection.\n",
    "\n",
    "pandas has already been imported as pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162779ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean for each group\n",
    "df.groupby('Class').mean()\n",
    "\n",
    "# Implement a rule for stating which cases are flagged as fraud\n",
    "df['flag_as_fraud'] = np.where(np.logical_and(df.V1 < -3, df.V3 < -5), 1, 0)\n",
    "\n",
    "# Create a crosstab of flagged fraud cases versus the actual fraud cases\n",
    "print(pd.crosstab(df.Class, df.flag_as_fraud, rownames=['Actual Fraud'], colnames=['Flagged Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c118cd",
   "metadata": {},
   "source": [
    "#### Using ML classification to catch fraud\n",
    "\n",
    "In this exercise you'll see what happens when you use a simple machine learning model on our credit card data instead.\n",
    "\n",
    "Do you think you can beat those results? Remember, you've predicted 22 out of 50 fraud cases, and had 16 false positives.\n",
    "\n",
    "So with that in mind, let's implement a Logistic Regression model. If you have taken the class on supervised learning in Python, you should be familiar with this model. If not, you might want to refresh that at this point. But don't worry, you'll be guided through the structure of the machine learning model.\n",
    "\n",
    "The X and y variables are available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit a logistic regression model to our data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Obtain model predictions\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "# Print the classifcation report and confusion matrix\n",
    "print('Classification report:\\n', classification_report(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)\n",
    "print('Confusion matrix:\\n', conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f66364f",
   "metadata": {},
   "source": [
    "#### Logistic regression combined with SMOTE\n",
    "\n",
    "In this exercise, you're going to take the Logistic Regression model from the previous exercise, and combine that with a SMOTE resampling method. We'll show you how to do that efficiently by using a pipeline that combines the resampling method with the model in one go. First, you need to define the pipeline that you're going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the pipeline module we need for this from imblearn\n",
    "from imblearn.pipeline import Pipeline \n",
    "\n",
    "# Define which resampling method and which ML model to use in the pipeline\n",
    "resampling = SMOTE()\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\n",
    "pipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', model)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6560e2",
   "metadata": {},
   "source": [
    "#### Using a pipeline\n",
    "\n",
    "Now that you have our pipeline defined, aka combining a logistic regression with a SMOTE method, let's run it on the data. You can treat the pipeline as if it were a single machine learning model. Our data X and y are already defined, and the pipeline is defined in the previous exercise. Are you curious to find out what the model results are? Let's give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e6403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split your data X and y, into a training and a test set and fit the pipeline onto the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data \n",
    "pipeline.fit(X_train, y_train) \n",
    "predicted = pipeline.predict(X_test)\n",
    "\n",
    "# Obtain the results from the classification report and confusion matrix \n",
    "print('Classifcation report:\\n', classification_report(y_test, predicted))\n",
    "conf_mat = confusion_matrix(y_true=y_test, y_pred=predicted)\n",
    "print('Confusion matrix:\\n', conf_mat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
